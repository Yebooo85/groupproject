{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "Random Forest - G_Score Best Parameters: {'bootstrap': True, 'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "Random Forest - G_Score MSE: 0.12345463708504927\n",
      "Random Forest - G_Score R²: 0.49971242704295404\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "Random Forest - E_Score Best Parameters: {'bootstrap': True, 'max_depth': 5, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "Random Forest - E_Score MSE: 0.48063745192218976\n",
      "Random Forest - E_Score R²: 0.2153040947254108\n",
      "Fitting 5 folds for each of 162 candidates, totalling 810 fits\n",
      "XGBoost - ESG_Score Best Parameters: {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.9}\n",
      "XGBoost - ESG_Score MSE: 0.5777116418163732\n",
      "XGBoost - ESG_Score R²: 0.1309139614085023\n",
      "Fitting 5 folds for each of 162 candidates, totalling 810 fits\n",
      "XGBoost - E_Score Best Parameters: {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.9}\n",
      "XGBoost - E_Score MSE: 0.47405121322105315\n",
      "XGBoost - E_Score R²: 0.22605688670879454\n",
      "Fitting 5 folds for each of 162 candidates, totalling 810 fits\n",
      "XGBoost - S_Score Best Parameters: {'colsample_bytree': 1.0, 'learning_rate': 0.2, 'max_depth': 10, 'n_estimators': 200, 'subsample': 0.9}\n",
      "XGBoost - S_Score MSE: 0.0442149408731774\n",
      "XGBoost - S_Score R²: -0.03207512518373323\n",
      "Fitting 5 folds for each of 162 candidates, totalling 810 fits\n",
      "XGBoost - G_Score Best Parameters: {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 10, 'n_estimators': 200, 'subsample': 0.8}\n",
      "XGBoost - G_Score MSE: 0.13949212137192488\n",
      "XGBoost - G_Score R²: 0.43472212550660616\n",
      "Fusion Model - ESG_Score MSE: 52.025459155195264\n",
      "Fusion Model - ESG_Score R²: -77.2649975010612\n",
      "Fusion Model - E_Score MSE: 0.4742379841385903\n",
      "Fusion Model - E_Score R²: 0.22575196171048306\n",
      "Fusion Model - S_Score MSE: 3.6409751945797795\n",
      "Fusion Model - S_Score R²: -83.98846443140683\n",
      "Fusion Model - G_Score MSE: 0.12568325112416157\n",
      "Fusion Model - G_Score R²: 0.4906811914813657\n",
      "Voting Regressor - ESG_Score MSE: 0.31858196163010616\n",
      "Voting Regressor - ESG_Score R²: 0.5207381763516152\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "import joblib\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, VotingRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('D:/DSS5201/esg_data.csv')\n",
    "data = data.drop(columns=['CompanyName', 'Industry', 'Sector'])  # Remove non-numeric columns\n",
    "\n",
    "# Handle missing values (imputation with median)\n",
    "imputer = SimpleImputer(strategy='median')  # Using median instead of mean\n",
    "data = pd.DataFrame(imputer.fit_transform(data), columns=data.columns)\n",
    "\n",
    "# Features and target variables\n",
    "X = data.drop(columns=['ESG_Score', 'E_Score', 'S_Score', 'G_Score'])\n",
    "y = data[['ESG_Score', 'E_Score', 'S_Score', 'G_Score']]\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# PCA for dimensionality reduction\n",
    "pca = PCA(n_components=0.95)  # Keep 95% variance\n",
    "X_scaled = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define hyperparameter grids for Random Forest and XGBoost\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [5, 7, 10],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 0.9, 1],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Initialize models\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "xgb = XGBRegressor(random_state=42)\n",
    "ridge = Ridge()\n",
    "lasso = Lasso()\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 1. Random Forest Model Training & Evaluation for G_Score and E_Score only\n",
    "best_rf_models = {}\n",
    "\n",
    "for target in ['G_Score', 'E_Score']:  # Only G_Score and E_Score for Random Forest\n",
    "    grid_search_rf = GridSearchCV(estimator=rf, param_grid=param_grid_rf, cv=5, n_jobs=-1, verbose=2, scoring='neg_mean_squared_error')\n",
    "    grid_search_rf.fit(X_train, y_train[target])\n",
    "    \n",
    "    print(f\"Random Forest - {target} Best Parameters:\", grid_search_rf.best_params_)\n",
    "    best_rf_models[target] = grid_search_rf.best_estimator_\n",
    "    \n",
    "    # Train and evaluate the model\n",
    "    y_pred_rf = best_rf_models[target].predict(X_test)\n",
    "    mse_rf = mean_squared_error(y_test[target], y_pred_rf)\n",
    "    r2_rf = r2_score(y_test[target], y_pred_rf)\n",
    "    \n",
    "    print(f'Random Forest - {target} MSE: {mse_rf}')\n",
    "    print(f'Random Forest - {target} R²: {r2_rf}')\n",
    "    \n",
    "    # Save the model\n",
    "    joblib.dump(best_rf_models[target], f'best_rf_model_{target}.pkl')\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 2. XGBoost Model Training & Evaluation for All Targets\n",
    "best_xgb_models = {}\n",
    "\n",
    "for target in y.columns:\n",
    "    grid_search_xgb = GridSearchCV(estimator=xgb, param_grid=param_grid_xgb, cv=5, n_jobs=-1, verbose=2, scoring='neg_mean_squared_error')\n",
    "    grid_search_xgb.fit(X_train, y_train[target])\n",
    "    \n",
    "    print(f\"XGBoost - {target} Best Parameters:\", grid_search_xgb.best_params_)\n",
    "    best_xgb_models[target] = grid_search_xgb.best_estimator_\n",
    "    \n",
    "    # Train and evaluate the model\n",
    "    y_pred_xgb = best_xgb_models[target].predict(X_test)\n",
    "    mse_xgb = mean_squared_error(y_test[target], y_pred_xgb)\n",
    "    r2_xgb = r2_score(y_test[target], y_pred_xgb)\n",
    "    \n",
    "    print(f'XGBoost - {target} MSE: {mse_xgb}')\n",
    "    print(f'XGBoost - {target} R²: {r2_xgb}')\n",
    "    \n",
    "    # Save the model\n",
    "    joblib.dump(best_xgb_models[target], f'best_xgb_model_{target}.pkl')\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 3. Fusion Model: Weighted Average of Random Forest & XGBoost Predictions\n",
    "weights_rf = 0.5  # Adjust weight for Random Forest\n",
    "weights_xgb = 0.5  # Adjust weight for XGBoost\n",
    "\n",
    "y_pred_fusion = {}\n",
    "for target in y.columns:\n",
    "    # Only use Random Forest for G_Score and E_Score\n",
    "    rf_model = best_rf_models.get(target, None)\n",
    "    xgb_model = best_xgb_models[target]\n",
    "    \n",
    "    # Ensure that the Random Forest model is only included if it exists\n",
    "    if rf_model:\n",
    "        y_pred_fusion[target] = weights_rf * rf_model.predict(X_test) + weights_xgb * xgb_model.predict(X_test)\n",
    "    else:\n",
    "        y_pred_fusion[target] = weights_xgb * xgb_model.predict(X_test)\n",
    "    \n",
    "    # Evaluate the fusion model\n",
    "    mse_fusion = mean_squared_error(y_test[target], y_pred_fusion[target])\n",
    "    r2_fusion = r2_score(y_test[target], y_pred_fusion[target])\n",
    "    \n",
    "    print(f'Fusion Model - {target} MSE: {mse_fusion}')\n",
    "    print(f'Fusion Model - {target} R²: {r2_fusion}')\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 4. Voting Regressor: Combine Models Using Voting\n",
    "# Include only models that are available for the ESG_Score\n",
    "voting_estimators = []\n",
    "\n",
    "# Add Random Forest for ESG_Score if it's trained\n",
    "if 'ESG_Score' in best_rf_models:\n",
    "    voting_estimators.append(('rf', best_rf_models['ESG_Score']))\n",
    "\n",
    "# Always add XGBoost for ESG_Score\n",
    "voting_estimators.append(('xgb', best_xgb_models['ESG_Score']))\n",
    "\n",
    "# Also add Ridge and Lasso as additional models\n",
    "voting_estimators.append(('ridge', ridge))\n",
    "voting_estimators.append(('lasso', lasso))\n",
    "\n",
    "# Create the Voting Regressor with the selected models\n",
    "voting_model = VotingRegressor(estimators=voting_estimators)\n",
    "\n",
    "# Train the Voting Regressor only on ESG_Score for now\n",
    "voting_model.fit(X_train, y_train['ESG_Score'])  # Train only on ESG_Score for voting model\n",
    "\n",
    "y_pred_voting = voting_model.predict(X_test)\n",
    "mse_voting = mean_squared_error(y_test['ESG_Score'], y_pred_voting)\n",
    "r2_voting = r2_score(y_test['ESG_Score'], y_pred_voting)\n",
    "\n",
    "print(f'Voting Regressor - ESG_Score MSE: {mse_voting}')\n",
    "print(f'Voting Regressor - ESG_Score R²: {r2_voting}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value at Risk (VaR) at 95% confidence level: -0.6208915258450635\n",
      "ESG Adjusted Beta: 1.0187199999999998\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# 1. Load Data\n",
    "# Original financial data\n",
    "financial_data = pd.DataFrame({\n",
    "    \"CompanyName\": [\"Johnson & Johnson\", \"Johnson & Johnson\", \"Johnson & Johnson\", \"Boston Scientific Corporation\", \n",
    "                    \"Boston Scientific Corporation\", \"Boston Scientific Corporation\", \"UnitedHealth Group Inc.\", \n",
    "                    \"UnitedHealth Group Inc.\", \"UnitedHealth Group Inc.\"],\n",
    "    \"Industry\": [\"Drug Manufacturers - Major\", \"Drug Manufacturers - Major\", \"Drug Manufacturers - Major\", \n",
    "                 \"Medical Appliances & Equipment\", \"Medical Appliances & Equipment\", \"Medical Appliances & Equipment\", \n",
    "                 \"Health Care Plans\", \"Health Care Plans\", \"Health Care Plans\"],\n",
    "    \"Sector\": [\"Healthcare\"] * 9,\n",
    "    \"Year\": [2022, 2021, 2020, 2022, 2021, 2020, 2022, 2021, 2020],\n",
    "    \"AnnualEarning_Billion\": [15.83, 21.35, 22.96, 1.99, 1.14, 1.08, 29.11, 26.34, 22.31],\n",
    "    \"MarketCap_Billion\": [461.84, 450.35, 414.3, 84.69, 66.27, 60.53, 495.37, 472.94, 332.73],\n",
    "    \"Revenue\": [85.16, 91.15, 93.78, 14.24, 12.68, 11.89, 367.53, 322.13, 285.27],\n",
    "    \"Expenses\": [69.33, 69.8, 70.82, 12.25, 11.54, 10.81, 338.42, 295.79, 262.96]\n",
    "})\n",
    "\n",
    "# Load ESG data\n",
    "esg_data = pd.read_csv('D:/DSS5201/esg_data.csv')\n",
    "\n",
    "# 2. Merge Datasets\n",
    "combined_data = pd.merge(financial_data, esg_data[['CompanyName', 'Year', 'ESG_Score']], on=['CompanyName', 'Year'], how='inner')\n",
    "\n",
    "# VaR Calculation\n",
    "market_cap_returns = combined_data['MarketCap_Billion'].pct_change().dropna()\n",
    "confidence_level = 0.95\n",
    "var_95 = np.percentile(market_cap_returns, (1 - confidence_level) * 100)\n",
    "print(\"Value at Risk (VaR) at 95% confidence level:\", var_95)\n",
    "\n",
    "# ESG-Adjusted Beta\n",
    "initial_beta = 1.2\n",
    "average_esg_score = combined_data['ESG_Score'].mean()\n",
    "esg_adjusted_beta = initial_beta * (1 - average_esg_score / 100)\n",
    "print(\"ESG Adjusted Beta:\", esg_adjusted_beta)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
