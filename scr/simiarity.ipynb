{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# 存指标\n",
    "csv_data = pd.read_excel('../dictionary.xlsx')  # 假设CSV文件名为target_metrics.csv\n",
    "unique_metrics = csv_data['metric'].dropna().unique().tolist()  # 获取唯一值列表\n",
    "unique_metrics_df = pd.DataFrame(unique_metrics, columns=['Unique Metrics'])\n",
    "unique_metrics_df.to_csv('unique_metrics.csv', index=False)\n",
    "print(\"唯一值已保存为 unique_metrics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "筛选完成，结果已保存为 filtered_data.json\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF向量化 精度较低，快\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 读取JSON文件\n",
    "with open('../json/grouped_data_full.json', 'r') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "# 读取CSV文件\n",
    "csv_data = pd.read_excel('../dictionary.xlsx')  # 假设CSV文件名为target_metrics.csv\n",
    "unique_keys = csv_data['key'].dropna().unique().tolist()  # 提取唯一的key列表\n",
    "\n",
    "# 提取JSON中的指标（metric字段）\n",
    "json_metrics = [item[\"metric\"] for item in json_data]\n",
    "\n",
    "# 使用TF-IDF向量化来计算相似度\n",
    "vectorizer = TfidfVectorizer().fit(json_metrics + unique_keys)\n",
    "json_vectors = vectorizer.transform(json_metrics)\n",
    "key_vectors = vectorizer.transform(unique_keys)\n",
    "\n",
    "# 设置相似度阈值\n",
    "similarity_threshold = 0.1\n",
    "filtered_data = []\n",
    "\n",
    "# 遍历JSON中的每个metric\n",
    "for i, json_metric in enumerate(json_metrics):\n",
    "    # 计算该JSON指标与所有唯一key的相似度\n",
    "    similarities = cosine_similarity(json_vectors[i], key_vectors).flatten()\n",
    "    \n",
    "    # 检查是否有相似度大于阈值的key\n",
    "    matched_key_indices = [j for j, sim in enumerate(similarities) if sim >= similarity_threshold]\n",
    "    \n",
    "    # 如果找到匹配的key\n",
    "    if matched_key_indices:\n",
    "        # 找到相似度最高的key\n",
    "        best_match_index = matched_key_indices[similarities[matched_key_indices].argmax()]\n",
    "        best_key = unique_keys[best_match_index]\n",
    "        best_similarity = similarities[best_match_index]\n",
    "        \n",
    "        # 获取匹配到的key对应的metric\n",
    "        standard_metric = csv_data.loc[csv_data['key'] == best_key, 'metric'].iloc[0]\n",
    "        \n",
    "        # 添加匹配到的key和metric，相似度，并保留原数据\n",
    "        filtered_entry = json_data[i].copy()\n",
    "        filtered_entry['key_word'] = best_key\n",
    "        filtered_entry['standard_metric'] = standard_metric\n",
    "        filtered_entry['similarities'] = best_similarity\n",
    "        filtered_data.append(filtered_entry)\n",
    "\n",
    "# 将结果保存为新的JSON文件\n",
    "with open('../json/filtered_data_full.json', 'w') as f:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
    "    json.dump(filtered_data, f, indent=4)\n",
    "\n",
    "print(\"筛选完成，结果已保存为 filtered_data.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating JSON metrics embeddings: 100%|██████████| 15/15 [00:05<00:00,  2.77it/s]\n",
      "Calculating target label embeddings: 100%|██████████| 434/434 [02:35<00:00,  2.79it/s]\n",
      "Calculating similarities: 100%|██████████| 15/15 [00:00<00:00, 272.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "筛选完成，结果已保存为 filtered_data.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# bert 嵌入 高精度基于语义，慢\n",
    "import json\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm  # 用于进度条\n",
    "\n",
    "# 加载BERT模型和分词器\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# 定义函数获取BERT嵌入并显示进度条\n",
    "def get_bert_embeddings_with_progress(text_list, description=\"Calculating embeddings\"):\n",
    "    embeddings = []\n",
    "    for text in tqdm(text_list, desc=description):\n",
    "        inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=128, padding='max_length')\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        embedding = outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
    "        embeddings.append(embedding)\n",
    "    return embeddings\n",
    "\n",
    "# 加载JSON和目标数据\n",
    "with open('../json/grouped_data_full.json', 'r') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "# 读取CSV文件\n",
    "csv_data = pd.read_excel('../dictionary.xlsx') \n",
    "target_labels = csv_data['key'].dropna().unique().tolist()\n",
    "\n",
    "# 提取JSON中的metric并获取BERT嵌入\n",
    "json_metrics = [item[\"metric\"] for item in json_data]\n",
    "json_embeddings = get_bert_embeddings_with_progress(json_metrics, \"Calculating JSON metrics embeddings\")\n",
    "target_embeddings = get_bert_embeddings_with_progress(target_labels, \"Calculating target label embeddings\")\n",
    "\n",
    "# 设置相似度阈值\n",
    "similarity_threshold = 0.7  # \n",
    "filtered_data = []\n",
    "\n",
    "# 比较每个JSON metric和目标标签的相似度，并显示进度条\n",
    "for i, json_metric in tqdm(enumerate(json_metrics), desc=\"Calculating similarities\", total=len(json_metrics)):\n",
    "    similarities = cosine_similarity([json_embeddings[i]], target_embeddings).flatten()\n",
    "    max_similarity_index = similarities.argmax()\n",
    "    max_similarity = similarities[max_similarity_index]\n",
    "    \n",
    "    # 如果相似度超过阈值，则保留该数据\n",
    "    if max_similarity >= similarity_threshold:\n",
    "        matched_label = target_labels[max_similarity_index]\n",
    "        standard_metric = csv_data.loc[csv_data['key'] == matched_label, 'metric'].iloc[0]\n",
    "        \n",
    "        filtered_entry = json_data[i].copy()\n",
    "        filtered_entry['key_word'] = matched_label\n",
    "        filtered_entry['standard_metric'] = standard_metric\n",
    "        filtered_entry['similarities'] = float(max_similarity)  # 转换为float类型\n",
    "        filtered_data.append(filtered_entry)\n",
    "\n",
    "# 将结果保存为新的JSON文件\n",
    "with open('filtered_data.json', 'w') as f:\n",
    "    json.dump(filtered_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"筛选完成，结果已保存为 filtered_data.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
