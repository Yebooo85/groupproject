{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ariaH\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForTokenClassification: ['deberta.embeddings.position_embeddings.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight']\n",
      "- This IS expected if you are initializing DebertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Extracting ESG Data: 100%|██████████| 123/123 [00:53<00:00,  2.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to ../output_metric/esg_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import csv\n",
    "\n",
    "# 加载DeBERTa模型和分词器\n",
    "model_name = \"microsoft/deberta-base\"  # 只需引用，transformers会自动下载模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "# 初始化NER pipeline\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# 读取清洗过的TXT文件\n",
    "def read_text_file(txt_file_path):\n",
    "    with open(txt_file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "def split_text(text, max_len=512):\n",
    "    # 将文本分段处理，以适应BERT输入限制\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    for sentence in sentences:\n",
    "        if len(current_chunk) + len(sentence) < max_len:\n",
    "            current_chunk += \" \" + sentence\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    return chunks\n",
    "\n",
    "# 提取ESG数据，增加进度条显示\n",
    "def extract_esg_data(text_chunks):\n",
    "    esg_data = []\n",
    "    for chunk in tqdm(text_chunks, desc=\"Extracting ESG Data\"):\n",
    "        entities = nlp(chunk)\n",
    "        current_data = {}\n",
    "        for entity in entities:\n",
    "            label = entity[\"entity\"]\n",
    "            word = entity[\"word\"]\n",
    "            # 根据标签分类存储\n",
    "            if label == \"LABEL_INDICATOR\":  # 替换为DeBERTa模型的指标标签\n",
    "                current_data[\"indicator\"] = word\n",
    "            elif label == \"LABEL_VALUE\":  # 替换为DeBERTa模型的数据值标签\n",
    "                current_data[\"value\"] = word\n",
    "            elif label == \"LABEL_UNIT\":  # 替换为DeBERTa模型的单位标签\n",
    "                current_data[\"unit\"] = word\n",
    "            # 如果当前数据项完整，保存到结果中\n",
    "            if \"indicator\" in current_data and \"value\" in current_data and \"unit\" in current_data:\n",
    "                esg_data.append(current_data)\n",
    "                current_data = {}\n",
    "    return esg_data\n",
    "\n",
    "# 保存提取的数据到CSV\n",
    "def save_to_csv(esg_data, output_csv_path):\n",
    "    keys = [\"indicator\", \"value\", \"unit\"]\n",
    "    with open(output_csv_path, 'w', newline='', encoding='utf-8') as output_file:\n",
    "        dict_writer = csv.DictWriter(output_file, fieldnames=keys)\n",
    "        dict_writer.writeheader()\n",
    "        dict_writer.writerows(esg_data)\n",
    "    print(f\"Data saved to {output_csv_path}\")\n",
    "\n",
    "# 使用示例\n",
    "txt_file_path = '../txt/AML.txt'\n",
    "output_csv_path = '../output_metric/esg_data.csv'\n",
    "\n",
    "# 读取和提取文本\n",
    "text = read_text_file(txt_file_path)\n",
    "# text_chunks = [text]  # 假设不分段，直接传入整个文本\n",
    "text_chunks = split_text(text)\n",
    "\n",
    "# 提取ESG数据并显示进度\n",
    "esg_data = extract_esg_data(text_chunks)\n",
    "\n",
    "# 保存到CSV\n",
    "save_to_csv(esg_data, output_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'LABEL_0', 1: 'LABEL_1'}\n"
     ]
    }
   ],
   "source": [
    "print(model.config.id2label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# 将数据转为BIO格式\n",
    "def convert_to_bio(data):\n",
    "    bio_data = []\n",
    "    \n",
    "    for item in data:\n",
    "        sentence_tokens = []\n",
    "        sentence_labels = []\n",
    "        \n",
    "        # Process the metric (indicator)\n",
    "        if item[\"metric\"]:\n",
    "            metric_tokens = item[\"metric\"].split()\n",
    "            sentence_tokens.extend(metric_tokens)\n",
    "            # 标记第一个词为B-INDICATOR，其余为I-INDICATOR\n",
    "            sentence_labels.append(\"B-INDICATOR\")\n",
    "            sentence_labels.extend([\"I-INDICATOR\"] * (len(metric_tokens) - 1))\n",
    "        \n",
    "        # Process the value\n",
    "        if item[\"value\"]:\n",
    "            value_tokens = str(item[\"value\"]).split()\n",
    "            sentence_tokens.extend(value_tokens)\n",
    "            # 标记第一个词为B-VALUE\n",
    "            sentence_labels.append(\"B-VALUE\")\n",
    "            sentence_labels.extend([\"I-VALUE\"] * (len(value_tokens) - 1))\n",
    "        \n",
    "        # Process the unit\n",
    "        if item[\"unit\"]:\n",
    "            unit_tokens = item[\"unit\"].split()\n",
    "            sentence_tokens.extend(unit_tokens)\n",
    "            # 标记第一个词为B-UNIT，其余为I-UNIT\n",
    "            sentence_labels.append(\"B-UNIT\")\n",
    "            sentence_labels.extend([\"I-UNIT\"] * (len(unit_tokens) - 1))\n",
    "        \n",
    "        # Append sentence tokens and labels to bio_data\n",
    "        bio_sentence = list(zip(sentence_tokens, sentence_labels))\n",
    "        bio_data.append(bio_sentence)\n",
    "    \n",
    "    return bio_data\n",
    "\n",
    "# 读取JSON文件\n",
    "with open(\"../json/grouped_data_full1.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# 转换为BIO格式\n",
    "bio_data = convert_to_bio(data)\n",
    "\n",
    "# 保存为BIO格式的文本文件\n",
    "with open(\"../output/bio_data.txt\", \"w\") as file:\n",
    "    for sentence in bio_data:\n",
    "        for word, label in sentence:\n",
    "            file.write(f\"{word} {label}\\n\")\n",
    "        file.write(\"\\n\")  # 每个句子之间用空行分隔\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, ClassLabel, Sequence\n",
    "\n",
    "# 加载BIO格式数据集\n",
    "def load_bio_dataset(file_path):\n",
    "    dataset = load_dataset(\"text\", data_files=file_path)\n",
    "\n",
    "    # 解析BIO数据格式并转换为NER任务的格式\n",
    "    def tokenize_and_align_labels(example):\n",
    "        tokens = []\n",
    "        labels = []\n",
    "        for line in example[\"text\"].split(\"\\n\"):\n",
    "            if line.strip():\n",
    "                word, label = line.split()\n",
    "                tokens.append(word)\n",
    "                labels.append(label)\n",
    "        return {\"tokens\": tokens, \"ner_tags\": labels}\n",
    "\n",
    "    # 应用解析函数并转换数据集\n",
    "    dataset = dataset.map(tokenize_and_align_labels, remove_columns=[\"text\"])\n",
    "\n",
    "    # 将标签转换为整数类型\n",
    "    label_list = sorted(set(sum(dataset[\"train\"][\"ner_tags\"], [])))  # 获取所有的标签\n",
    "    label_to_id = {label: i for i, label in enumerate(label_list)}\n",
    "    \n",
    "    def label_to_int(example):\n",
    "        example[\"ner_tags\"] = [label_to_id[label] for label in example[\"ner_tags\"]]\n",
    "        return example\n",
    "\n",
    "    dataset = dataset.map(label_to_int)\n",
    "    \n",
    "    # 设置标签格式\n",
    "    features = dataset[\"train\"].features.copy()\n",
    "    features[\"ner_tags\"] = Sequence(feature=ClassLabel(names=label_list))\n",
    "    dataset = dataset.cast(features)\n",
    "\n",
    "    return dataset, label_list\n",
    "\n",
    "# 加载并处理数据集\n",
    "file_path = \"bio_data.txt\"\n",
    "dataset, label_list = load_bio_dataset(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import DebertaTokenizer, DebertaForTokenClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# 1. Load and parse the JSON, BIO, and raw text files\n",
    "def load_json_data(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "def load_bio_data(filepath):\n",
    "    texts, labels = [], []\n",
    "    with open(filepath, 'r') as file:\n",
    "        text, label = [], []\n",
    "        for line in file:\n",
    "            if line.strip() == \"\":\n",
    "                if text:\n",
    "                    texts.append(\" \".join(text))\n",
    "                    labels.append(label)\n",
    "                    text, label = [], []\n",
    "                continue\n",
    "            \n",
    "            # 将BIO标签正确映射为数字\n",
    "            word, tag = line.strip().split()\n",
    "            text.append(word)\n",
    "            \n",
    "            # 转换标签\n",
    "            if tag == 'O':\n",
    "                label.append(0)\n",
    "            elif 'B' in tag:\n",
    "                label.append(1)  # 假设B-*标签映射为1\n",
    "            elif 'I' in tag:\n",
    "                label.append(2)  # 假设I-*标签映射为2\n",
    "            \n",
    "        if text:  # 处理文件最后一行\n",
    "            texts.append(\" \".join(text))\n",
    "            labels.append(label)\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "def load_raw_text(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        return [line.strip() for line in file if line.strip()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "json_filepath = '../json/grouped_data_full1.json'\n",
    "bio_filepath = '../output/bio_data.txt'\n",
    "raw_text_filepath = '../output/1030_split.txt'\n",
    "\n",
    "# Load data\n",
    "json_data = load_json_data(json_filepath)\n",
    "bio_texts, bio_labels = load_bio_data(bio_filepath)\n",
    "raw_texts = load_raw_text(raw_text_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ariaH\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading file vocab.json from cache at C:\\Users\\ariaH/.cache\\huggingface\\hub\\models--microsoft--deberta-base\\snapshots\\0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\ariaH/.cache\\huggingface\\hub\\models--microsoft--deberta-base\\snapshots\\0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195\\merges.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\ariaH/.cache\\huggingface\\hub\\models--microsoft--deberta-base\\snapshots\\0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\ariaH/.cache\\huggingface\\hub\\models--microsoft--deberta-base\\snapshots\\0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195\\config.json\n",
      "Model config DebertaConfig {\n",
      "  \"_name_or_path\": \"microsoft/deberta-base\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"c2p\",\n",
      "    \"p2c\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"relative_attention\": true,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\ariaH/.cache\\huggingface\\hub\\models--microsoft--deberta-base\\snapshots\\0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195\\config.json\n",
      "Model config DebertaConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"c2p\",\n",
      "    \"p2c\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"relative_attention\": true,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\ariaH/.cache\\huggingface\\hub\\models--microsoft--deberta-base\\snapshots\\0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForTokenClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'deberta.embeddings.position_embeddings.weight', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "C:\\Users\\ariaH\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 18\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 9\n",
      "  Number of trainable parameters = 138604035\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "526ff668f08b40e38112b80ab50c9354",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2. Initialize tokenizer and model\n",
    "tokenizer = DebertaTokenizer.from_pretrained(\"microsoft/deberta-base\")\n",
    "model = DebertaForTokenClassification.from_pretrained(\"microsoft/deberta-base\", num_labels=3)\n",
    "\n",
    "# 3. Custom Dataset Class for labeled and unlabeled data\n",
    "class ESGDataset(Dataset):\n",
    "    def __init__(self, texts, labels=None):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = tokenizer(self.texts[idx], truncation=True, padding=\"max_length\", max_length=128, return_tensors=\"pt\")\n",
    "        item = {key: val.squeeze() for key, val in encoding.items()}\n",
    "        if self.labels:\n",
    "            labels = self.labels[idx] + [2] * (128 - len(self.labels[idx]))  # Pad labels to max length\n",
    "            item[\"labels\"] = torch.tensor(labels)\n",
    "        return item\n",
    "\n",
    "# Prepare datasets\n",
    "train_dataset = ESGDataset(bio_texts, bio_labels)  # BIO labeled data for supervised training\n",
    "unlabeled_dataset = ESGDataset(raw_texts)  # Unlabeled data for unsupervised pretraining\n",
    "\n",
    "# 4. Training parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"no\",#\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# 5. Trainer for supervised training with labeled data\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "# 6. Fine-tuning\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../model/fine_tuned_deberta\n",
      "Configuration saved in ../model/fine_tuned_deberta\\config.json\n",
      "Model weights saved in ../model/fine_tuned_deberta\\pytorch_model.bin\n",
      "tokenizer config file saved in ../model/fine_tuned_deberta\\tokenizer_config.json\n",
      "Special tokens file saved in ../model/fine_tuned_deberta\\special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('../model/fine_tuned_deberta\\\\tokenizer_config.json',\n",
       " '../model/fine_tuned_deberta\\\\special_tokens_map.json',\n",
       " '../model/fine_tuned_deberta\\\\vocab.json',\n",
       " '../model/fine_tuned_deberta\\\\merges.txt',\n",
       " '../model/fine_tuned_deberta\\\\added_tokens.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练完成后保存模型和分词器\n",
    "trainer.save_model(\"../model/fine_tuned_deberta\")\n",
    "tokenizer.save_pretrained(\"../model/fine_tuned_deberta\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import DebertaTokenizer, DebertaForTokenClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# 1. Load and parse the JSON, BIO, and raw text files\n",
    "def load_json_data(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "def load_bio_data(filepath):\n",
    "    texts, labels = [], []\n",
    "    with open(filepath, 'r') as file:\n",
    "        text, label = [], []\n",
    "        for line in file:\n",
    "            if line.strip() == \"\":\n",
    "                if text:\n",
    "                    texts.append(\" \".join(text))\n",
    "                    labels.append(label)\n",
    "                    text, label = [], []\n",
    "                continue\n",
    "            word, tag = line.strip().split()\n",
    "            text.append(word)\n",
    "            label.append(0 if tag == 'O' else 1 if 'B' in tag else 2)  # Convert B-I-O to 0-1-2\n",
    "        if text:\n",
    "            texts.append(\" \".join(text))\n",
    "            labels.append(label)\n",
    "    return texts, labels\n",
    "\n",
    "def load_raw_text(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        return [line.strip() for line in file if line.strip()]\n",
    "\n",
    "# File paths\n",
    "json_filepath = '/mnt/data/grouped_data_full1.json'\n",
    "bio_filepath = '/mnt/data/bio_data.txt'\n",
    "raw_text_filepath = '/mnt/data/1030_split.txt'\n",
    "\n",
    "# Load data\n",
    "json_data = load_json_data(json_filepath)\n",
    "bio_texts, bio_labels = load_bio_data(bio_filepath)\n",
    "raw_texts = load_raw_text(raw_text_filepath)\n",
    "\n",
    "# 2. Initialize tokenizer and model\n",
    "tokenizer = DebertaTokenizer.from_pretrained(\"microsoft/deberta-base\")\n",
    "model = DebertaForTokenClassification.from_pretrained(\"microsoft/deberta-base\", num_labels=3)\n",
    "\n",
    "# 3. Custom Dataset Class for labeled and unlabeled data\n",
    "class ESGDataset(Dataset):\n",
    "    def __init__(self, texts, labels=None):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = tokenizer(self.texts[idx], truncation=True, padding=\"max_length\", max_length=128, return_tensors=\"pt\")\n",
    "        item = {key: val.squeeze() for key, val in encoding.items()}\n",
    "        if self.labels:\n",
    "            labels = self.labels[idx] + [2] * (128 - len(self.labels[idx]))  # Pad labels to max length\n",
    "            item[\"labels\"] = torch.tensor(labels)\n",
    "        return item\n",
    "\n",
    "# Prepare datasets\n",
    "train_dataset = ESGDataset(bio_texts, bio_labels)  # BIO labeled data for supervised training\n",
    "unlabeled_dataset = ESGDataset(raw_texts)  # Unlabeled data for unsupervised pretraining\n",
    "\n",
    "# 4. Training parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# 5. Trainer for supervised training with labeled data\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "# 6. Fine-tuning\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_texts, bio_labels = load_bio_data(bio_filepath)\n",
    "print(bio_texts[:2])\n",
    "print(bio_labels[:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
