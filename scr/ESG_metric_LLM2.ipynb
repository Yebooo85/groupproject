{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# 将数据转为BIO格式\n",
    "def convert_to_bio(data):\n",
    "    bio_data = []\n",
    "    \n",
    "    for item in data:\n",
    "        sentence_tokens = []\n",
    "        sentence_labels = []\n",
    "        \n",
    "        # Process the metric (indicator)\n",
    "        if item[\"metric\"]:\n",
    "            metric_tokens = item[\"metric\"].split()\n",
    "            sentence_tokens.extend(metric_tokens)\n",
    "            # 标记第一个词为B-INDICATOR，其余为I-INDICATOR\n",
    "            sentence_labels.append(\"B-INDICATOR\")\n",
    "            sentence_labels.extend([\"I-INDICATOR\"] * (len(metric_tokens) - 1))\n",
    "        \n",
    "        # Process the value\n",
    "        if item[\"value\"]:\n",
    "            value_tokens = str(item[\"value\"]).split()\n",
    "            sentence_tokens.extend(value_tokens)\n",
    "            # 标记第一个词为B-VALUE\n",
    "            sentence_labels.append(\"B-VALUE\")\n",
    "            sentence_labels.extend([\"I-VALUE\"] * (len(value_tokens) - 1))\n",
    "        \n",
    "        # Process the unit\n",
    "        if item[\"unit\"]:\n",
    "            unit_tokens = item[\"unit\"].split()\n",
    "            sentence_tokens.extend(unit_tokens)\n",
    "            # 标记第一个词为B-UNIT，其余为I-UNIT\n",
    "            sentence_labels.append(\"B-UNIT\")\n",
    "            sentence_labels.extend([\"I-UNIT\"] * (len(unit_tokens) - 1))\n",
    "        \n",
    "        # Append sentence tokens and labels to bio_data\n",
    "        bio_sentence = list(zip(sentence_tokens, sentence_labels))\n",
    "        bio_data.append(bio_sentence)\n",
    "    \n",
    "    return bio_data\n",
    "\n",
    "# 读取JSON文件\n",
    "with open(\"../json/grouped_data_full1.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# 转换为BIO格式\n",
    "bio_data = convert_to_bio(data)\n",
    "\n",
    "# 保存为BIO格式的文本文件\n",
    "with open(\"../output/bio_data2.txt\", \"w\") as file:\n",
    "    for sentence in bio_data:\n",
    "        for word, label in sentence:\n",
    "            file.write(f\"{word} {label}\\n\")\n",
    "        file.write(\"\\n\")  # 每个句子之间用空行分隔\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import DebertaTokenizer, DebertaForTokenClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# 定义标签映射\n",
    "label2id = {\"O\": 0, \"B-INDICATOR\": 1, \"I-INDICATOR\": 2, \"B-VALUE\": 3, \"I-VALUE\": 4, \"B-UNIT\": 5, \"I-UNIT\": 6}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "def load_json_data(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "def load_raw_text(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        return [line.strip() for line in file if line.strip()]\n",
    "\n",
    "# 1. 加载并解析BIO格式数据\n",
    "def load_bio_data(filepath):\n",
    "    texts, labels = [], []\n",
    "    with open(filepath, 'r') as file:\n",
    "        text, label = [], []\n",
    "        for line in file:\n",
    "            if line.strip() == \"\":\n",
    "                if text:\n",
    "                    texts.append(\" \".join(text))\n",
    "                    labels.append(label)\n",
    "                    text, label = [], []\n",
    "                continue\n",
    "            \n",
    "            # 从文件中读取单词和BIO标签，并将标签映射为数字\n",
    "            word, tag = line.strip().split()\n",
    "            text.append(word)\n",
    "            label.append(label2id[tag])  # 使用自定义的label2id映射\n",
    "            \n",
    "        if text:  # 处理文件最后一行\n",
    "            texts.append(\" \".join(text))\n",
    "            labels.append(label)\n",
    "    return texts, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'B-INDICATOR',\n",
       " 2: 'I-INDICATOR',\n",
       " 3: 'B-VALUE',\n",
       " 4: 'I-VALUE',\n",
       " 5: 'B-UNIT',\n",
       " 6: 'I-UNIT'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "json_filepath = '../json/grouped_data_full1.json'\n",
    "bio_filepath = '../output/bio_data.txt'\n",
    "raw_text_filepath = '../txt/AML.txt'\n",
    "\n",
    "# Load data\n",
    "json_data = load_json_data(json_filepath)\n",
    "bio_texts, bio_labels = load_bio_data(bio_filepath)\n",
    "raw_texts = load_raw_text(raw_text_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ariaH\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading file vocab.json from cache at C:\\Users\\ariaH/.cache\\huggingface\\hub\\models--microsoft--deberta-base\\snapshots\\0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\ariaH/.cache\\huggingface\\hub\\models--microsoft--deberta-base\\snapshots\\0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195\\merges.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\ariaH/.cache\\huggingface\\hub\\models--microsoft--deberta-base\\snapshots\\0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\ariaH/.cache\\huggingface\\hub\\models--microsoft--deberta-base\\snapshots\\0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195\\config.json\n",
      "Model config DebertaConfig {\n",
      "  \"_name_or_path\": \"microsoft/deberta-base\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"c2p\",\n",
      "    \"p2c\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"relative_attention\": true,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\ariaH/.cache\\huggingface\\hub\\models--microsoft--deberta-base\\snapshots\\0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195\\config.json\n",
      "Model config DebertaConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"c2p\",\n",
      "    \"p2c\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"relative_attention\": true,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\ariaH/.cache\\huggingface\\hub\\models--microsoft--deberta-base\\snapshots\\0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForTokenClassification: ['lm_predictions.lm_head.dense.bias', 'deberta.embeddings.position_embeddings.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "# 加载BIO格式数据\n",
    "bio_texts, bio_labels = load_bio_data(bio_filepath)\n",
    "\n",
    "# 2. 自定义Dataset\n",
    "class ESGDataset(Dataset):\n",
    "    def __init__(self, texts, labels=None):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = tokenizer(self.texts[idx], truncation=True, padding=\"max_length\", max_length=128, return_tensors=\"pt\")\n",
    "        item = {key: val.squeeze() for key, val in encoding.items()}\n",
    "        if self.labels:\n",
    "            labels = self.labels[idx] + [0] * (128 - len(self.labels[idx]))  # 使用0填充标签\n",
    "            item[\"labels\"] = torch.tensor(labels)\n",
    "        return item\n",
    "\n",
    "# 加载分词器和模型\n",
    "model_name = \"microsoft/deberta-base\"\n",
    "tokenizer = DebertaTokenizer.from_pretrained(model_name)\n",
    "model = DebertaForTokenClassification.from_pretrained(model_name, num_labels=len(label2id))\n",
    "\n",
    "# 设置标签映射\n",
    "model.config.label2id = label2id\n",
    "model.config.id2label = id2label\n",
    "\n",
    "# 3. 准备训练数据和Trainer\n",
    "train_dataset = ESGDataset(bio_texts, bio_labels)\n",
    "unlabeled_dataset = ESGDataset(raw_texts)\n",
    "\n",
    "# 4. Training parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    # logging_dir='./logs',  # 启用日志\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# 5. Trainer for supervised training with labeled data\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ariaH\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 18\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 9\n",
      "  Number of trainable parameters = 138607111\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "169d7b20d12640c6a3a8ec57026cde86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to ./fine_tuned_deberta\n",
      "Configuration saved in ./fine_tuned_deberta\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 75.0504, 'train_samples_per_second': 0.72, 'train_steps_per_second': 0.12, 'train_loss': 0.6539309819539388, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./fine_tuned_deberta\\pytorch_model.bin\n",
      "tokenizer config file saved in ./fine_tuned_deberta\\tokenizer_config.json\n",
      "Special tokens file saved in ./fine_tuned_deberta\\special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_deberta\\\\tokenizer_config.json',\n",
       " './fine_tuned_deberta\\\\special_tokens_map.json',\n",
       " './fine_tuned_deberta\\\\vocab.json',\n",
       " './fine_tuned_deberta\\\\merges.txt',\n",
       " './fine_tuned_deberta\\\\added_tokens.json')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6. 开始微调并保存模型\n",
    "trainer.train()\n",
    "trainer.save_model(\"../model/fine_tuned_deberta\")\n",
    "tokenizer.save_pretrained(\"../model/fine_tuned_deberta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json\n",
      "loading file merges.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file ./fine_tuned_deberta\\config.json\n",
      "Model config DebertaConfig {\n",
      "  \"_name_or_path\": \"./fine_tuned_deberta\",\n",
      "  \"architectures\": [\n",
      "    \"DebertaForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"B-INDICATOR\",\n",
      "    \"2\": \"I-INDICATOR\",\n",
      "    \"3\": \"B-VALUE\",\n",
      "    \"4\": \"I-VALUE\",\n",
      "    \"5\": \"B-UNIT\",\n",
      "    \"6\": \"I-UNIT\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"B-INDICATOR\": 1,\n",
      "    \"B-UNIT\": 5,\n",
      "    \"B-VALUE\": 3,\n",
      "    \"I-INDICATOR\": 2,\n",
      "    \"I-UNIT\": 6,\n",
      "    \"I-VALUE\": 4,\n",
      "    \"O\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"c2p\",\n",
      "    \"p2c\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"relative_attention\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file ./fine_tuned_deberta\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing DebertaForTokenClassification.\n",
      "\n",
      "All the weights of DebertaForTokenClassification were initialized from the model checkpoint at ./fine_tuned_deberta.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DebertaForTokenClassification for predictions without further training.\n",
      "Processing Text Chunks: 100%|██████████| 123/123 [00:52<00:00,  2.36it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# 加载保存的微调模型和分词器\n",
    "model_path = \"../model/fine_tuned_deberta\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "\n",
    "# 初始化NER pipeline\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "\n",
    "# 读取新的TXT文件\n",
    "def read_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    return text\n",
    "\n",
    "# 分段处理长文本\n",
    "def split_text_by_sentences(text, max_len=512):\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|\\!)\\s', text)\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        if len(current_chunk) + len(sentence) < max_len:\n",
    "            current_chunk += \" \" + sentence\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# 提取结构化数据\n",
    "def extract_structured_data(text_chunks):\n",
    "    structured_data = []\n",
    "    for chunk in tqdm(text_chunks, desc=\"Processing Text Chunks\"):\n",
    "        entities = ner_pipeline(chunk)\n",
    "        current_data = {}\n",
    "        for entity in entities:\n",
    "            label_id = entity[\"entity_group\"]  # 直接获取数值标签\n",
    "            word = entity[\"word\"]\n",
    "            \n",
    "            # 根据数值标签进行分类\n",
    "            if label_id == 1:  # B-METRIC\n",
    "                current_data[\"indicator\"] = word\n",
    "            elif label_id == 2:  # I-METRIC\n",
    "                if \"indicator\" in current_data:\n",
    "                    current_data[\"indicator\"] += \" \" + word\n",
    "            elif label_id == 3:  # B-VALUE\n",
    "                current_data[\"value\"] = word\n",
    "            elif label_id == 4:  # I-VALUE\n",
    "                if \"value\" in current_data:\n",
    "                    current_data[\"value\"] += \" \" + word\n",
    "            elif label_id == 5:  # B-UNIT\n",
    "                current_data[\"unit\"] = word\n",
    "            elif label_id == 6:  # I-UNIT\n",
    "                if \"unit\" in current_data:\n",
    "                    current_data[\"unit\"] += \" \" + word\n",
    "            \n",
    "            # 保存完整的结构化数据项\n",
    "            if \"indicator\" in current_data and \"value\" in current_data and \"unit\" in current_data:\n",
    "                structured_data.append(current_data)\n",
    "                current_data = {}\n",
    "    return structured_data\n",
    "    \n",
    "\n",
    "# 使用示例\n",
    "file_path = raw_text_filepath  # 新的报告TXT文件\n",
    "text = read_text_file(file_path)\n",
    "text_chunks = split_text_by_sentences(text)  # 分段处理文本，避免超长输入\n",
    "\n",
    "# 提取结构化数据\n",
    "structured_data = extract_structured_data(text_chunks)\n",
    "\n",
    "# 输出结果\n",
    "for item in structured_data:\n",
    "    print(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structured_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Net profit 1.9 S$ million', 'Total greenhouse gas ( GHG ) emissions 346 tonnes CO2e']\n",
      "[[1, 2, 3, 5, 6], [1, 2, 2, 2, 2, 2, 2, 3, 5, 6]]\n"
     ]
    }
   ],
   "source": [
    "bio_texts, bio_labels = load_bio_data(bio_filepath)\n",
    "print(bio_texts[:2])\n",
    "print(bio_labels[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
